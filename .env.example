# PDF MCQ Generator - Environment Variables
# Copy this file to .env and fill in your actual values
# DO NOT commit .env file to GitHub - it contains sensitive information

# ============================================
# API Provider Configuration
# ============================================

# OpenAI API Configuration
OPENAI_API_KEY=your_openai_api_key_here

# OpenRouter API Configuration (Recommended for cost-effective models)
OPENROUTER_API_KEY=your_openrouter_api_key_here
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# DeepSeek API Configuration
DEEPSEEK_API_KEY=your_deepseek_api_key_here
DEEPSEEK_BASE_URL=https://api.deepseek.com/v1

# ============================================
# Application Configuration
# ============================================

# Flask Configuration
FLASK_ENV=production
FLASK_DEBUG=False
SECRET_KEY=your_secret_key_here

# Upload Configuration
MAX_UPLOAD_SIZE=524288000  # 500MB in bytes (increased from 50MB to support larger PDFs)
UPLOAD_FOLDER=uploads

# ============================================
# MCQ Generation Configuration
# ============================================

# Default Model Provider (openai, openrouter, deepseek)
DEFAULT_MODEL_PROVIDER=openrouter
DEFAULT_MODEL_NAME=deepseek/deepseek-chat

# Generation Parameters
DEFAULT_QUESTION_COUNT=5
DEFAULT_DIFFICULTY=medium

# ============================================
# Offline Generation (Optional)
# ============================================

# Enable offline MCQ generation (requires additional dependencies)
ENABLE_OFFLINE_GENERATION=False

# ============================================
# Logging Configuration
# ============================================

LOG_LEVEL=INFO
LOG_FILE=logs/app.log

# ============================================
# Vercel Specific Configuration
# ============================================

# Set to True when deploying to Vercel
VERCEL_DEPLOYMENT=True

# Temporary file handling for serverless
USE_TEMP_DIRECTORY=True

